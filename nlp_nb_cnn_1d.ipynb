{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('airline_sentiment')['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = df.groupby('airline_sentiment')['text'].nunique().sort_values(ascending=False).reset_index(drop=True)\n",
    "group = []\n",
    "values = []\n",
    "for k,v in df_iter.items():\n",
    "    group.append(k)\n",
    "    values.append(v)\n",
    "df_nunique = pd.DataFrame({'group' : group , 'values':values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x=\"group\", \n",
    "    y=\"values\", \n",
    "    data=df_nunique, \n",
    "    estimator=sum, \n",
    "    ci=None, \n",
    "    color='#69b3a2'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# since these are tweets, remove the @s to the airline and other users\n",
    "def remove_statics(text):\n",
    "    text = re.sub('@([A-Za-z0-9a_]+)', '' , text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = str(text)\n",
    "    return text.translate(str.maketrans('' , '' , string.punctuation)).lower()\n",
    "\n",
    "def remove_non_alnum(text):\n",
    "    text = str(text)\n",
    "    text_list = [ch for ch in text.split() if ch.isalnum()]\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.split()\n",
    "    return ' '.join([e for e in text if e not in stop_words])\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatized_text = []\n",
    "    for word in text.split():\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def pos_tag(tokenized_text):\n",
    "    tokenized_text = tokenized_text.split()\n",
    "    return nltk.pos_tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_text'] = df['text'].apply(remove_stopwords)\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(remove_statics)\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lemmatize)\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply((remove_punctuations))\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(remove_non_alnum)\n",
    "df['text_pos'] = df['text'].apply(pos_tag)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['airline_sentiment'].unique()\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_labels = le.fit(labels)\n",
    "print(encoded_labels.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.label_map = {'neutral' : 0 , 'positive': 1 , 'negative':2}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['preprocessed_text']\n",
    "        label = self.df.iloc[idx]['airline_sentiment']\n",
    "        return self.label_map[label] , text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "MAX_LEN = 50\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(df['preprocessed_text'].tolist())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[1] for item in batch]\n",
    "    target = [item[0] for item in batch]\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    data = pad_sequences(data, maxlen=MAX_LEN)\n",
    "    data = torch.tensor(data, dtype=torch.long)\n",
    "    target = torch.tensor(target, dtype=torch.long)\n",
    "    return data , target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(df) - 1)\n",
    "test_size = len(df) - train_size\n",
    "\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(Tweets(df), [train_size, test_size])\n",
    "\n",
    "train = DataLoader(train_dataset,collate_fn=collate_fn , batch_size=128, shuffle=True)\n",
    "valid = DataLoader(test_dataset,collate_fn=collate_fn, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [2 , 3 , 4]\n",
    "class TextClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextClassification, self).__init__()\n",
    "        n_classes = len(le.classes_)\n",
    "\n",
    "        self.embedding = nn.Embedding(MAX_FEATURES, 100)\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(MAX_LEN , 256 , kernel_size = filter_sizes[0], stride=1)\n",
    "        self.pool_1 = nn.AdaptiveMaxPool1d(2048)\n",
    "        \n",
    "        # self.conv2 = nn.Conv1d(MAX_LEN , 256 , kernel_size = filter_sizes[1] , stride= 1)\n",
    "        # self.padding2 = nn.ConstantPad1d(padding=1 , value=0)\n",
    "        # self.pool_2 = nn.MaxPool1d(kernel_size=filter_sizes[1], stride=1)\n",
    "\n",
    "        # self.conv3 = nn.Conv1d(MAX_LEN , 256 , kernel_size = filter_sizes[0] , stride=1)\n",
    "        # self.pool_3 = nn.MaxPool1d(kernel_size=filter_sizes[0] , stride=1)\n",
    "        \n",
    "        self.linear1 = nn.Linear(2048 , n_classes)\n",
    "        self.norm1 = nn.BatchNorm1d(256)\n",
    "        self.linearf = nn.Linear(n_classes, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed_result = self.embedding(x)\n",
    "\n",
    "        conv1 = self.conv1(embed_result)\n",
    "        conv1 = F.relu(conv1)\n",
    "        pool1 = self.pool_1(conv1)\n",
    "\n",
    "        # conv2 = self.conv2(embed_result)\n",
    "        # conv2 = F.relu(conv2)\n",
    "        # pool2 = self.pool_2(self.padding2(conv2))\n",
    "\n",
    "        # conv3 = self.conv3(embed_result)\n",
    "        # conv3 = F.relu(conv3)\n",
    "        # pool3 = self.pool_3(conv3)\n",
    "        \n",
    "        # union = torch.concat((pool1 , pool2 , pool3) , 1)\n",
    "\n",
    "        x = self.linear1(pool1)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm1(x)\n",
    "        x = self.linearf(x)\n",
    "        \n",
    "        return F.softmax(x , dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.ModuleList):\n",
    "\n",
    "   def __init__(self):\n",
    "      super(TextClassifier, self).__init__()\n",
    "\n",
    "      # Parameters regarding text preprocessing\n",
    "      self.seq_len = MAX_LEN\n",
    "      self.num_words = MAX_FEATURES\n",
    "      self.embedding_size = 100\n",
    "      \n",
    "      # Dropout definition\n",
    "      self.dropout = nn.Dropout(0.25)\n",
    "      \n",
    "      # CNN parameters definition\n",
    "      # Kernel sizes\n",
    "      self.kernel_1 = 2\n",
    "      self.kernel_2 = 3\n",
    "      self.kernel_3 = 4\n",
    "      self.kernel_4 = 5\n",
    "      \n",
    "      # Output size for each convolution\n",
    "      self.out_size = 32\n",
    "      # Number of strides for each convolution\n",
    "      self.stride = 1\n",
    "      \n",
    "      # Embedding layer definition\n",
    "      self.embedding = nn.Embedding(self.num_words + 1, self.embedding_size, padding_idx=0)\n",
    "      \n",
    "      # Convolution layers definition\n",
    "      self.conv_1 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_1, self.stride)\n",
    "      self.conv_2 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_2, self.stride)\n",
    "      self.conv_3 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_3, self.stride)\n",
    "      self.conv_4 = nn.Conv1d(self.seq_len, self.out_size, self.kernel_4, self.stride)\n",
    "      \n",
    "      # Max pooling layers definition\n",
    "      self.pool_1 = nn.MaxPool1d(self.kernel_1, self.stride)\n",
    "      self.pool_2 = nn.MaxPool1d(self.kernel_2, self.stride)\n",
    "      self.pool_3 = nn.MaxPool1d(self.kernel_3, self.stride)\n",
    "      self.pool_4 = nn.MaxPool1d(self.kernel_4, self.stride)\n",
    "      \n",
    "      # Fully connected layer definition\n",
    "      self.fc = nn.Linear(380, 1)\n",
    "\n",
    "\n",
    "\n",
    "   def forward(self, x):\n",
    "         # Sequence of tokes is filterd through an embedding layer\n",
    "         x = self.embedding(x)\n",
    "         \n",
    "         # Convolution layer 1 is applied\n",
    "         x1 = self.conv_1(x)\n",
    "         x1 = torch.relu(x1)\n",
    "         x1 = self.pool_1(x1)\n",
    "         \n",
    "         # Convolution layer 2 is applied\n",
    "         x2 = self.conv_2(x)\n",
    "         x2 = torch.relu((x2))\n",
    "         x2 = self.pool_2(x2)\n",
    "   \n",
    "         # Convolution layer 3 is applied\n",
    "         x3 = self.conv_3(x)\n",
    "         x3 = torch.relu(x3)\n",
    "         x3 = self.pool_3(x3)\n",
    "         \n",
    "         # Convolution layer 4 is applied\n",
    "         x4 = self.conv_4(x)\n",
    "         x4 = torch.relu(x4)\n",
    "         x4 = self.pool_4(x4)\n",
    "         \n",
    "         # The output of each convolutional layer is concatenated into a unique vector\n",
    "         union = torch.cat((x1, x2, x3, x4), 2)\n",
    "         # union = union.reshape(union.size(0), -1)\n",
    "\n",
    "         # The \"flattened\" vector is passed through a fully connected layer\n",
    "         out = self.fc(union)\n",
    "         # Dropout is applied\t\t\n",
    "         out = self.dropout(out)\n",
    "         # Activation function is applied\n",
    "         out = torch.sigmoid(out)\n",
    "         \n",
    "         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextClassifier(\n",
      "  (dropout): Dropout(p=0.25, inplace=False)\n",
      "  (embedding): Embedding(5001, 100, padding_idx=0)\n",
      "  (conv_1): Conv1d(1024, 32, kernel_size=(2,), stride=(1,))\n",
      "  (conv_2): Conv1d(1024, 32, kernel_size=(3,), stride=(1,))\n",
      "  (conv_3): Conv1d(1024, 32, kernel_size=(4,), stride=(1,))\n",
      "  (conv_4): Conv1d(1024, 32, kernel_size=(5,), stride=(1,))\n",
      "  (pool_1): MaxPool1d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_2): MaxPool1d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_3): MaxPool1d(kernel_size=4, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (pool_4): MaxPool1d(kernel_size=5, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc): Linear(in_features=380, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = TextClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "def train_loop(train):\n",
    "    total_acc, total_count = 0, 0\n",
    "    for idx, (x_batch, y_batch) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(x_batch)\n",
    "        predicted_label = predicted_label.squeeze()\n",
    "        loss = criterion(predicted_label, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(-1) == y_batch).sum().item()\n",
    "        total_count += y_batch.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid):\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (x_batch, y_batch) in tqdm(enumerate(valid)):\n",
    "            predicted_label = model(x_batch)\n",
    "            predicted_label = predicted_label.squeeze()\n",
    "            total_acc += (predicted_label.argmax(-1) == y_batch).sum().item()\n",
    "            total_count += y_batch.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:44,  1.14s/it]\n",
      "23it [00:18,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | train_acc: 0.17 | val_acc:  0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:48,  1.17s/it]\n",
      "23it [00:19,  1.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | train_acc: 0.39 | val_acc:  0.42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:46,  1.16s/it]\n",
      "23it [00:19,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | train_acc: 0.33 | val_acc:  0.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:47,  1.17s/it]\n",
      "23it [00:19,  1.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | train_acc: 0.28 | val_acc:  0.24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [01:48,  1.18s/it]\n",
      "23it [00:19,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | train_acc: 0.25 | val_acc:  0.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LR = 0.0001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_acc = train_loop(train)\n",
    "    model.eval()    \n",
    "    val_acc = evaluate(valid)\n",
    "    print('Epoch ' , epoch + 1, '|' , 'train_acc:' , \"{:.2f}\".format(train_acc) , '|' ,'val_acc: ', \"{:.2f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model , 'model_cnn.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kosig')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66761006ad8bd18f178cbe8c4411e89165011137b898da0879d8d636d9cdd01e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
