{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import spacy\n",
    "import re\n",
    "import torch.nn.functional as F\n",
    "import seaborn as sns\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn import preprocessing\n",
    "from torch import nn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Tweets.csv')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('airline_sentiment')['text'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iter = df.groupby('airline_sentiment')['text'].nunique().sort_values(ascending=False).reset_index(drop=True)\n",
    "group = []\n",
    "values = []\n",
    "for k,v in df_iter.items():\n",
    "    group.append(k)\n",
    "    values.append(v)\n",
    "df_nunique = pd.DataFrame({'group' : group , 'values':values})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(\n",
    "    x=\"group\", \n",
    "    y=\"values\", \n",
    "    data=df_nunique, \n",
    "    estimator=sum, \n",
    "    ci=None, \n",
    "    color='#69b3a2'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# since these are tweets, remove the @s to the airline and other users\n",
    "def remove_statics(text):\n",
    "    text = re.sub('@([A-Za-z0-9a_]+)', '' , text)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    text = str(text)\n",
    "    return text.translate(str.maketrans('' , '' , string.punctuation)).lower()\n",
    "\n",
    "def remove_non_alnum(text):\n",
    "    text = str(text)\n",
    "    text_list = [ch for ch in text.split() if ch.isalnum()]\n",
    "    return ' '.join(text_list)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = text.split()\n",
    "    return ' '.join([e for e in text if e not in stop_words])\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmatized_text = []\n",
    "    for word in text.split():\n",
    "        lemmatized_text.append(lemmatizer.lemmatize(word))\n",
    "    return ' '.join(lemmatized_text)\n",
    "\n",
    "def pos_tag(tokenized_text):\n",
    "    tokenized_text = tokenized_text.split()\n",
    "    return nltk.pos_tag(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['preprocessed_text'] = df['text'].apply(remove_stopwords)\n",
    "df['preprocessed_text'] = df['text'].apply(remove_statics)\n",
    "df['preprocessed_text'] = df['preprocessed_text'].apply(lemmatize)\n",
    "# df['preprocessed_text'] = df['preprocessed_text'].apply((remove_punctuations))\n",
    "# df['preprocessed_text'] = df['preprocessed_text'].apply(remove_non_alnum)\n",
    "df['text_pos'] = df['text'].apply(pos_tag)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['airline_sentiment'].unique()\n",
    "le = preprocessing.LabelEncoder()\n",
    "encoded_labels = le.fit(labels)\n",
    "print(encoded_labels.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tweets(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.label_map = {'neutral' : 0 , 'positive': 1 , 'negative':2}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.df.iloc[idx]['preprocessed_text']\n",
    "        label = self.df.iloc[idx]['airline_sentiment']\n",
    "        return self.label_map[label] , text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_FEATURES = 5000\n",
    "MAX_LEN = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES)\n",
    "tokenizer.fit_on_texts(df['preprocessed_text'].tolist())\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = [item[1] for item in batch]\n",
    "    target = [item[0] for item in batch]\n",
    "    data = tokenizer.texts_to_sequences(data)\n",
    "    data = pad_sequences(data, maxlen=MAX_LEN)\n",
    "    data = torch.tensor(data, dtype=torch.int64)\n",
    "    target = torch.tensor(target, dtype=torch.int64)\n",
    "    return data , target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import WeightedRandomSampler , random_split\n",
    "\n",
    "train_size = int(0.8 * len(df) - 1)\n",
    "test_size = len(df) - train_size\n",
    "\n",
    "train_dataset, test_dataset = random_split(Tweets(df), [train_size, test_size])\n",
    "\n",
    "sampler = WeightedRandomSampler([0.5 , 0.5 , 0.5] , num_samples=train_size)\n",
    "\n",
    "train = DataLoader(train_dataset,collate_fn=collate_fn , batch_size=64, sampler=sampler)\n",
    "valid = DataLoader(test_dataset,collate_fn=collate_fn, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 32\n",
    "class TextClassification(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(TextClassification, self).__init__()\n",
    "        n_classes = len(le.classes_)\n",
    "        self.embedding = nn.Embedding(MAX_FEATURES, MAX_LEN)\n",
    "        self.lstm = nn.LSTM(MAX_LEN , 256, batch_first=True , bidirectional = True)\n",
    "        self.dropout = nn.Dropout(p=0.25)\n",
    "        self.linear1 = nn.Linear(512 , n_classes)\n",
    "        self.linearf = nn.Linear(n_classes, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed_result = self.embedding(x)\n",
    "        output,_ = self.lstm(embed_result)\n",
    "        dropout = self.dropout(output)\n",
    "        x = self.linear1(dropout)\n",
    "        x = F.relu(x)\n",
    "        x = self.linearf(x)\n",
    "        x = F.softmax(x,dim=1)\n",
    "        print(x[1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TextClassification()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def train_loop(train):\n",
    "    total_acc, total_count = 0, 0\n",
    "    for idx, (x_batch, y_batch) in tqdm(enumerate(train)):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(x_batch)\n",
    "        predicted_label = predicted_label.squeeze()\n",
    "        loss = criterion(predicted_label, y_batch)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(-1) == y_batch).sum().item()\n",
    "        total_count += y_batch.size(0)\n",
    "    return total_acc/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid):\n",
    "    total_acc, total_count = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (x_batch, y_batch) in tqdm(enumerate(valid)):\n",
    "            predicted_label = model(x_batch)\n",
    "            predicted_label = predicted_label.squeeze()\n",
    "            total_acc += (predicted_label.argmax(-1) == y_batch).sum().item()\n",
    "            total_count += y_batch.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "LR = 0.0001\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_start_time = time.time()\n",
    "    model.train()\n",
    "    train_acc = train_loop(train)\n",
    "    model.eval()    \n",
    "    val_acc = evaluate(valid)\n",
    "    print('Epoch ' , epoch + 1, '|' , 'train_acc:' , \"{:.2f}\".format(train_acc) , '|' ,'val_acc: ', \"{:.2f}\".format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model , 'model_lstm.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('kosig')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66761006ad8bd18f178cbe8c4411e89165011137b898da0879d8d636d9cdd01e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
